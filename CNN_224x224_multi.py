# -*- coding: utf-8 -*-
"""Nueva version 224x224 multi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O1fS-NedPr9lo1ilU_BVob3o68IcslRw

##Importaciones
"""

from keras.models import Sequential
from keras.preprocessing import image
from keras import regularizers
from keras.layers import (
    Dense,
    Dropout,
    Flatten,
    Conv2D,
    MaxPooling2D
    )
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    confusion_matrix,
    accuracy_score,
    recall_score,
    precision_score,
    f1_score
    )
from tensorflow.keras.optimizers import Adam
from tensorflow.image import rgb_to_grayscale
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.utils import plot_model
import tensorflow as tf
import matplotlib.pyplot as plt
from tqdm import tqdm
import pandas as pd
import numpy as np
import os
from keras.utils import to_categorical
from keras.layers import BatchNormalization
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
import cv2
from keras.optimizers import SGD
from tensorflow.keras.regularizers import l2
from sklearn.model_selection import KFold

"""##Montar Google Drive"""

from google.colab import drive
drive.mount('/content/drive')

# Cargar las imágenes procesadas
X = np.load('/content/drive/MyDrive/Dataset Desbalanceado/processed_images.npy')

# Leer el archivo CSV
data = pd.read_csv('/content/drive/MyDrive/Dataset Desbalanceado/dataset.csv')

# Obtener la columna 'label'
y = data.iloc[:, 1].values  # Etiquetas

# Dividir los datos en conjuntos de entrenamiento y prueba
# Puedes ajustar el test_size según tus necesidades
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

X_train = X_train.reshape(-1, 224, 224, 1)
X_test = X_test.reshape(-1, 224, 224, 1)

# Convertir las etiquetas a representación one-hot
y_train = to_categorical(y_train, num_classes=4)
y_test = to_categorical(y_test, num_classes=4)

"""##Crear el modelo, Entrenar y Cross Vall"""

# Cargar el archivo CSV que contiene las rutas de las imágenes y las etiquetas
data = pd.read_csv('/content/drive/MyDrive/Dataset Desbalanceado/dataset.csv')

# Cargar imágenes procesadas
processed_images_file = '/content/drive/MyDrive/Dataset Desbalanceado/processed_images.npy'
X = np.load(processed_images_file)

# Cargar las etiquetas desde el DataFrame
y = data.iloc[:, 1].values  # Suponiendo que la segunda columna contiene las etiquetas

# Parámetros del modelo
inp_shape = (224, 224, 1)
act = 'relu'
drop = 0.5
kernal_reg = regularizers.l2(0.001)
epochs = 50
batch_size = 32

# Convertir las etiquetas a representación one-hot
num_classes = len(np.unique(y))
y = to_categorical(y, num_classes=num_classes)

# Configuración para K-Fold Cross Validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Almacenar resultados de la validación cruzada
cv_scores = []
cv_losses = []
cv_recalls = []
cv_f1_scores = []

for train_index, val_index in kf.split(X):
    X_train, X_val = X[train_index], X[val_index]
    y_train, y_val = y[train_index], y[val_index]

    # Crear el modelo
    model = Sequential()
    model.add(Conv2D(64, kernel_size=(3, 3), activation=act, input_shape=inp_shape,
                      kernel_regularizer=kernal_reg,
                      kernel_initializer='he_uniform', padding='same', name='Input_Layer'))
    model.add(BatchNormalization())
    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

    model.add(Conv2D(128, kernel_size=(3, 3), activation=act,
                      kernel_regularizer=kernal_reg,
                      kernel_initializer='he_uniform', padding='same'))
    model.add(BatchNormalization())
    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

    model.add(Flatten())
    model.add(Dense(64, activation=act))
    model.add(Dropout(drop))
    model.add(Dense(num_classes, activation='softmax', name='Output_Layer'))

    # Compilar el modelo
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

    # Callbacks para guardar el modelo con la mejor precisión
    early_stopping = EarlyStopping(monitor='val_accuracy', verbose=1, patience=5, min_delta=0.005)
    # Callbacks para guardar el modelo con la mejor precisión y detener el entrenamiento temprano
    model_filename = '/content/drive/MyDrive/Dataset Desbalanceado/modelo_fold_{fold_number}.keras'
    checkpoint = ModelCheckpoint(model_filename.format(fold_number=len(cv_scores)),
                                 monitor='val_accuracy',
                                 verbose=1,
                                 save_best_only=True)

    # Entrenar el modelo
    history = model.fit(X_train, y_train,
                        validation_data=(X_val, y_val),
                        epochs=epochs,
                        batch_size=batch_size,
                        callbacks=[early_stopping, checkpoint],
                        verbose=1)

# Evaluar el modelo en el conjunto de validación y almacenar la precisión y pérdida
    scores = model.evaluate(X_val, y_val)

    cv_scores.append(scores[1])  # Almacenar precisión (accuracy)
    cv_losses.append(scores[0])   # Almacenar pérdida (loss)

    # Calcular predicciones y métricas adicionales
    y_true = y_val.argmax(axis=1)  # Etiquetas reales
    y_pred = model.predict(X_val).argmax(axis=1)  # Predicciones del modelo

    recall = recall_score(y_true, y_pred, average='macro')
    f1 = f1_score(y_true, y_pred, average='macro')

    cv_recalls.append(recall)
    cv_f1_scores.append(f1)

# Imprimir resultados de la validación cruzada
print(f'Precisión media en validación cruzada: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}')
print(f'Pérdida media en validación cruzada: {np.mean(cv_losses):.4f} ± {np.std(cv_losses):.4f}')
print(f'Recall medio en validación cruzada: {np.mean(cv_recalls):.4f} ± {np.std(cv_recalls):.4f}')
print(f'F1-Score medio en validación cruzada: {np.mean(cv_f1_scores):.4f} ± {np.std(cv_f1_scores):.4f}')