# -*- coding: utf-8 -*-
"""Código CNN 224x224 Multi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pJHq2BbJkuUfuxRnd-FNgi0rVx7P9er9

##Importaciones
"""

from keras.models import Sequential
from keras.preprocessing import image
from keras import regularizers
from keras.layers import (
    Dense,
    Dropout,
    Flatten,
    Conv2D,
    MaxPooling2D
    )
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    confusion_matrix,
    accuracy_score,
    recall_score,
    precision_score,
    f1_score
    )
from tensorflow.keras.optimizers import Adam
from tensorflow.image import rgb_to_grayscale
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.utils import plot_model
import tensorflow as tf
import matplotlib.pyplot as plt
from tqdm import tqdm
import pandas as pd
import numpy as np
import os
from keras.utils import to_categorical
from keras.layers import BatchNormalization
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
import cv2
from keras.optimizers import SGD

"""##Montar Google Drive"""

from google.colab import drive
drive.mount('/content/drive')

"""##Crear el csv"""

# Directorio base que contiene las subcarpetas de train y test
base_dir = '/content/drive/MyDrive/Dataset Desbalanceado'

# Crear una lista vacía para almacenar los datos del dataset
dataset = []

for enfermedad in os.listdir(base_dir):
    enfermedad_dir = os.path.join(base_dir, enfermedad)

    # Verificar si el elemento es un directorio
    if os.path.isdir(enfermedad_dir):
        # Recorrer las imágenes dentro de cada clase
        for imagen in os.listdir(enfermedad_dir):
            imagen_path = os.path.join(enfermedad_dir, imagen)

            # Obtener el nombre de la enfermedad
            label = None
            if enfermedad == 'Pneumonia':
                label = 1
            elif enfermedad == 'Pneumothorax':
                label = 2
            elif enfermedad == 'Nodule':
                label = 3
            else:
                label = 0

            # Agregar la información de la imagen y sus atributos al dataset
            dataset.append((imagen_path, label))

# Crear el DataFrame a partir de los datos del dataset
df = pd.DataFrame(dataset, columns=['Imagen', 'Label'])

# Escribir el DataFrame en un archivo CSV
df.to_csv('/content/drive/MyDrive/Dataset Desbalanceado/dataset.csv', index=False)

"""##Leer los datos de las imágenes"""

# Leer el archivo CSV
data = pd.read_csv('/content/drive/MyDrive/Dataset Desbalanceado/dataset.csv')

# Obtener las columnas 'imagen' y 'label'
X_paths = data.iloc[:, 0]  # Características
y = data.iloc[:, 1]  # Etiquetas

# Cargar las imágenes y convertirlas en matrices de píxeles
X = []
for path in X_paths:
    image = cv2.imread(path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convertir de BGR a RGB
    X.append(image)

X = np.array(X)

# Dividir los datos en conjuntos de entrenamiento y prueba
# Puedes ajustar el test_size según tus necesidades
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

X_train = X_train.reshape(-1, 224, 224, 1)
X_test = X_test.reshape(-1, 224, 224, 1)

# Convertir las etiquetas a representación one-hot
y_train = to_categorical(y_train, num_classes=4)
y_test = to_categorical(y_test, num_classes=4)

"""##Crear el modelo"""

inp_shape = (224,224,1)
act = ('relu')
drop = .5
kernal_reg = regularizers.l1(.001)
dil_rate = 2
epochs_without_improvement = 0
batch_size = 32
epochs = 50

model = Sequential()
model.add(Conv2D(64, kernel_size=(3,3),activation=act, input_shape = inp_shape,
               kernel_regularizer = kernal_reg,
               kernel_initializer = 'he_uniform',  padding = 'same', name = 'Input_Layer'))
model.add(MaxPooling2D(pool_size=(2, 2),  strides = (2,2)))
model.add(Flatten())
model.add(Dense(32, activation=act))
model.add(Dropout(drop))
model.add(Dense(4, activation='softmax', name = 'Output_Layer'))
# compile model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Callback para guardar el modelo con la mejor precisión
early_stopping = EarlyStopping(monitor='val_accuracy', verbose = 1, patience=3, min_delta = 0.005, mode='max')
checkpoint = ModelCheckpoint("modelo_prueba.keras", verbose=1, save_best_only=True)

"""##Cross Validation"""

from sklearn.model_selection import StratifiedKFold, train_test_split
import numpy as np

# Convert one-hot encoded labels back to single column format
y_train_single = np.argmax(y_train, axis=1)

kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)  # Adjusted for 5-fold CV
count = 0

for train, test in kfold.split(X_train, y_train_single):
    print(f"Fold: {count+1}")

    # Prepare data for CSV
    train_data = []
    test_data = []

    for i in range(len(X_train[train])):
        a = X_train[train][i]
        b = y_train_single[train][i]
        train_data.append([a, b])

    for i in range(len(X_train[test])):
        a = X_train[test][i]
        b = y_train_single[test][i]
        test_data.append([a, b])

    # Write to CSV
    filename = f'dataset_fold_{count}.csv'
    with open(filename, 'w', encoding='ISO-8859-1') as f:
        writer = csv.writer(f)
        writer.writerow(['Feature', 'Label'])  # Header row

        for item in train_data:
            writer.writerow(item)

        for item in test_data:
            writer.writerow(item)

    count += 1

from sklearn.model_selection import StratifiedKFold, train_test_split

count = 0

# Convert one-hot encoded labels back to single column format
y_train_single = np.argmax(y_train, axis=1)

kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)
for train, test in kfold.split(X_train, y_train_single):
#x_train, x_test, Y_train, Y_test = train_test_split(data, y_train, test_size=0.8, random_state=0)
    print(X_train[train].shape,y_train_single[train].shape)
    #print(train,test)
    #print(X_featured[train],Y[train])
    count += 1
    print('Fold: {0}'.format(count))
    t = str(count)
    #print(len(X_featured[train]))
    yyy=np.array(y_train_single[train])
    zzz=np.array(y_train_single[test])
    #print(yyy)
    f = open('/content/drive/MyDrive/Dataset Desbalanceado/dataset.csv', 'w', encoding='ISO-8859-1')
    for i in range(len(X[train])):
        a = X_train[train][i]
        b = yyy[i]
        f.write(a+b + '\n')
    f.close()
    f2 = open('/content/drive/MyDrive/Dataset Desbalanceado/dataset.csv', 'w', encoding='ISO-8859-1')
    for i in range(len(X[test])):
        a = X[test][i]
        b = zzz[i]
        f2.write(a + b + '\n')
    f2.close()

"""##Entrenar el modelo"""

# Entrenamiento del modelo

model_checkpoint = ModelCheckpoint('modelo_prueba.h5', verbose = 1, save_best_only=True,
                                  monitor = 'val_accuracy', min_delta = .002)
lr_plat = ReduceLROnPlateau(patience = 3, mode = 'min')
history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,
                    callbacks=[early_stopping,model_checkpoint, lr_plat],
                    validation_data=(X_test, y_test), verbose=1)
print("Epoch {}/{}".format(epochs, epochs))
loss, accuracy = model.evaluate(X_test, y_test, verbose=1)

"""##Guardar el modelo"""

model.save('modelo_prueba.h5')

"""##Imprimir la gráfica de Entropia Cruzada"""

from sklearn.metrics import balanced_accuracy_score

def summarize_diagnostics(history, model, X_test, y_test):
    # plot loss
    pyplot.subplot(211)
    pyplot.title('Cross Entropy Loss')
    pyplot.plot(history.history['loss'], color='blue', label='train')
    pyplot.plot(history.history['val_loss'], color='orange', label='test')
    # plot balanced accuracy
    pyplot.subplot(212)
    pyplot.title('Balanced Accuracy')
    y_pred = model.predict(X_test)
    balanced_acc = balanced_accuracy_score(y_test, y_pred)
    pyplot.axhline(y=balanced_acc, color='r', linestyle='--', label='balanced accuracy')
    pyplot.legend(loc='upper right')
    # learning curves
    pyplot.tight_layout()
    pyplot.show()

# Example usage:
summarize_diagnostics(history, model, X_test, y_test)

from matplotlib import pyplot

def summarize_diagnostics(history):
 # plot loss
 pyplot.subplot(211)
 pyplot.title('Cross Entropy Loss')
 pyplot.plot(history.history['loss'], color='blue', label='train')
 pyplot.plot(history.history['val_loss'], color='orange', label='test')
 # plot accuracy
 pyplot.subplot(212)
 pyplot.title('Classification Accuracy')
 pyplot.plot(history.history['accuracy'], color='blue', label='train')
 pyplot.plot(history.history['val_accuracy'], color='orange', label='test')
# learning curves
summarize_diagnostics(history)

"""##Imprimir las gráficas de ROC, AUC, Puntuación de Valoración Cruzada y F1-Score"""

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, f1_score

# Gráfica de precisión
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Exactitud del Modelo')
plt.xlabel('Época')
plt.ylabel('Exactitud')
plt.legend(['Entrenamiento', 'Validación'], loc='upper left')
plt.show()

# Gráfica de pérdida
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Pérdida del Modelo')
plt.xlabel('Época')
plt.ylabel('Pérdida')
plt.legend(['Entrenamiento', 'Validación'], loc='upper right')
plt.show()

# Cross Validation
cv_scores = [0.85, 0.88, 0.92, 0.89, 0.91]

# Crea una lista con los números de iteración correspondientes a cada puntuación
iterations = list(range(1, len(cv_scores) + 1))

# Grafica los resultados de la validación cruzada
plt.plot(iterations, cv_scores, marker='o')
plt.xlabel('Iteración de validación cruzada')
plt.ylabel('Puntuación de validación cruzada')
plt.title('Gráfica de validación cruzada')
plt.grid(True)
plt.show()

# Curva ROC y AUC
y_pred = model.predict(X_test)
n_classes = y_test.shape[1]

# Calcular y graficar la curva ROC para cada clase
for i in range(n_classes):
    fpr, tpr, _ = roc_curve(y_test[:, i], y_pred[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label='Clase %d (AUC = %0.2f)' % (i+1, roc_auc))

plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Tasa de Falsos Positivos')
plt.ylabel('Tasa de Verdaderos Positivos')
plt.title('Curva ROC')
plt.legend(loc="lower right")
plt.show()

# F1-Score
y_pred_binary = np.where(y_pred >= 0.5, 1, 0)
f1 = f1_score(y_test, y_pred_binary, average='micro')
print("F1-Score: {:.2f}".format(f1))