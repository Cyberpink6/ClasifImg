# -*- coding: utf-8 -*-
"""Binario Pneumothorax.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_F6P6UoXqX_VQjgEliZEwIR2c_9p8erI

##Importaciones
"""

from keras.models import Sequential
from keras.preprocessing import image
from keras import regularizers
from keras.layers import (
    Dense,
    Dropout,
    Flatten,
    Conv2D,
    MaxPooling2D
    )
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    confusion_matrix,
    accuracy_score,
    recall_score,
    precision_score,
    f1_score
    )
from tensorflow.keras.optimizers import Adam
from tensorflow.image import rgb_to_grayscale
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
import tensorflow as tf
import matplotlib.pyplot as plt
from keras.preprocessing.image import ImageDataGenerator
from tqdm import tqdm
import pandas as pd
import numpy as np
import os
from keras.utils import to_categorical
from keras.layers import BatchNormalization
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
import cv2
from keras.optimizers import SGD

"""##Montar Google Drive"""

from google.colab import drive
drive.mount('/content/drive')

"""##Crear el csv"""

# Directorio base que contiene las subcarpetas de train y test
base_dir = '/content/drive/MyDrive/Binario Pneumothorax'

# Crear una lista vacía para almacenar los datos del dataset
dataset = []

for enfermedad in os.listdir(base_dir):
    enfermedad_dir = os.path.join(base_dir, enfermedad)

    # Verificar si el elemento es un directorio
    if os.path.isdir(enfermedad_dir):
        # Recorrer las imágenes dentro de cada clase
        for imagen in os.listdir(enfermedad_dir):
            imagen_path = os.path.join(enfermedad_dir, imagen)

            # Obtener el nombre de la enfermedad
            label = None
            if enfermedad == 'Pneumothorax':
                label = 1
            else:
                label = 0

            # Agregar la información de la imagen y sus atributos al dataset
            dataset.append((imagen_path, label))

# Crear el DataFrame a partir de los datos del dataset
df = pd.DataFrame(dataset, columns=['Imagen', 'Label'])

# Escribir el DataFrame en un archivo CSV
df.to_csv('/content/drive/MyDrive/Binario Pneumothorax/dataset.csv', index=False)

"""##Leer los datos de las imágenes"""

# Leer el archivo CSV
data = pd.read_csv('/content/drive/MyDrive/Binario Pneumothorax/dataset.csv')

# Obtener las columnas 'imagen' y 'label'
X_paths = data.iloc[:, 0]  # Características
y = data.iloc[:, 1]  # Etiquetas

# Cargar las imágenes y convertirlas en matrices de píxeles
X = []
for path in X_paths:
    image = cv2.imread(path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convertir de BGR a RGB
    X.append(image)

X = np.array(X)

# Dividir los datos en conjuntos de entrenamiento y prueba
# Puedes ajustar el test_size según tus necesidades
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.reshape(-1, 224, 224, 1)
X_test = X_test.reshape(-1, 224, 224, 1)

# Convertir las etiquetas a representación one-hot
y_train = to_categorical(y_train, num_classes=2)
y_test = to_categorical(y_test, num_classes=2)

"""##Crear el modelo"""

inp_shape = (224,224,1)
act = ('relu')
drop = .5
kernal_reg = regularizers.l1(.001)
dil_rate = 2
epochs_without_improvement = 0
batch_size = 32
epochs = 50

model = Sequential()
model.add(Conv2D(64, kernel_size=(3,3),activation=act, input_shape = inp_shape,
               kernel_regularizer = kernal_reg,
               kernel_initializer = 'he_uniform',  padding = 'same', name = 'Input_Layer'))
model.add(Dense(64, activation=act))
model.add(MaxPooling2D(pool_size=(2, 2),  strides = (2,2)))

model.add(Conv2D(64, (3, 3), activation=act, kernel_regularizer = kernal_reg, dilation_rate = dil_rate,
               kernel_initializer = 'he_uniform',padding = 'same'))
model.add(Dense(64, activation=act))
model.add(MaxPooling2D(pool_size=(2, 2), strides = (2,2)))

model.add(Conv2D(128, (3, 3), activation=act, kernel_regularizer = kernal_reg, dilation_rate = dil_rate,
               kernel_initializer = 'he_uniform',padding = 'same'))
model.add(Conv2D(128, (3, 3), activation=act, kernel_regularizer = kernal_reg, dilation_rate = dil_rate,
               kernel_initializer = 'he_uniform',padding = 'same'))
model.add(MaxPooling2D(pool_size=(2, 2), strides = (2,2)))
model.add(Flatten())
model.add(Dense(128, activation=act))
model.add(Dense(64, activation=act))
model.add(Dense(33, activation=act))
model.add(Dropout(drop))
model.add(Dense(2, activation='sigmoid', name = 'Output_Layer'))
# compile model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Callback para guardar el modelo con la mejor precisión
early_stopping = EarlyStopping(monitor='val_accuracy', verbose = 1, patience=3, min_delta = 0.005, mode='max')
checkpoint = ModelCheckpoint("modelo_prueba.h5", verbose=1, save_best_only=False)

"""##Entrenar el modelo"""

# Entrenamiento del modelo
for epoch in range(1):
    model_checkpoint = ModelCheckpoint('modelo_prueba.h5', verbose = 1, save_best_only=True,
                                  monitor = 'val_accuracy', min_delta = .002)
    lr_plat = ReduceLROnPlateau(patience = 3, mode = 'min')
    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,
                    callbacks=[early_stopping,model_checkpoint, lr_plat],
                    validation_data=(X_test, y_test), verbose=1)
    print("Epoch {}/{}".format(epochs, epochs))
    loss, accuracy = model.evaluate(X_test, y_test, verbose=1)

# Entrenamiento del modelo big
model = Sequential()
model.add(Conv2D(64, kernel_size=(3,3),activation=act, input_shape = inp_shape,
               kernel_regularizer = kernal_reg,
               kernel_initializer = 'he_uniform',  padding = 'same', name = 'Input_Layer'))
model.add(Dense(64, activation=act))
model.add(MaxPooling2D(pool_size=(2, 2),  strides = (2,2)))

model.add(Conv2D(64, (3, 3), activation=act, kernel_regularizer = kernal_reg, dilation_rate = dil_rate,
               kernel_initializer = 'he_uniform',padding = 'same'))
model.add(Dense(64, activation=act))
model.add(MaxPooling2D(pool_size=(2, 2), strides = (2,2)))

model.add(Conv2D(128, (3, 3), activation=act, kernel_regularizer = kernal_reg, dilation_rate = dil_rate,
               kernel_initializer = 'he_uniform',padding = 'same'))
model.add(Conv2D(128, (3, 3), activation=act, kernel_regularizer = kernal_reg, dilation_rate = dil_rate,
               kernel_initializer = 'he_uniform',padding = 'same'))
model.add(MaxPooling2D(pool_size=(2, 2), strides = (2,2)))
model.add(Flatten())
model.add(Dense(128, activation=act))
model.add(Dense(64, activation=act))
model.add(Dense(33, activation=act))
model.add(Dropout(drop))
model.add(Dense(4, activation='softmax', name = 'Output_Layer'))
# compile model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Entrenamiento del modelo mid
model = Sequential()

model.add(Conv2D(64, kernel_size=(3,3),activation=act, input_shape = inp_shape,
                     kernel_regularizer = kernal_reg,
                     kernel_initializer = 'he_uniform',  padding = 'same', name = 'Input_Layer'))
model.add(Dense(64, activation = 'relu'))
model.add(MaxPooling2D(pool_size=(2, 2),  strides = (2,2)))

model.add(Conv2D(64, (3, 3), activation=act, kernel_regularizer = kernal_reg, dilation_rate = dil_rate,
                     kernel_initializer = 'he_uniform',padding = 'same'))
model.add(Dense(64, activation = 'relu'))
model.add(MaxPooling2D(pool_size=(2, 2), strides = (2,2)))

model.add(Conv2D(128, (3, 3), activation=act, kernel_regularizer = kernal_reg, dilation_rate = dil_rate,
                     kernel_initializer = 'he_uniform',padding = 'same'))
model.add(Conv2D(128, (3, 3), activation=act, kernel_regularizer = kernal_reg, dilation_rate = dil_rate,
                     kernel_initializer = 'he_uniform',padding = 'same'))
model.add(MaxPooling2D(pool_size=(2, 2), strides = (2,2)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dropout(drop))
model.add(Dense(3, activation='sigmoid', name = 'Output_Layer'))
model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])


# Entrenamiento del modelo small
model = Sequential()
model.add(Conv2D(64, kernel_size=(3, 3), activation=act, input_shape=inp_shape, kernel_initializer='he_uniform', padding='same', name='Input_Layer'))
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

model.add(Conv2D(64, (3, 3), activation=act, kernel_initializer='he_uniform', padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

model.add(Conv2D(128, (3, 3), activation=act, kernel_initializer='he_uniform', padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

model.add(Flatten())
model.add(Dense(128, activation=act))
model.add(Dropout(drop))
model.add(Dense(2, activation='sigmoid', name='Output_Layer'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

"""##Guardar el modelo"""

model.save('modelo_binario_pneumonia.h5')

"""##Imprimir la gráfica de Entropia Cruzada"""

from sklearn.metrics import balanced_accuracy_score

def summarize_diagnostics(history, model, X_test, y_test):
    # plot loss
    pyplot.subplot(211)
    pyplot.title('Cross Entropy Loss')
    pyplot.plot(history.history['loss'], color='blue', label='train')
    pyplot.plot(history.history['val_loss'], color='orange', label='test')
    # plot balanced accuracy
    pyplot.subplot(212)
    pyplot.title('Balanced Accuracy')
    y_pred = model.predict(X_test)
    balanced_acc = balanced_accuracy_score(y_test, y_pred)
    pyplot.axhline(y=balanced_acc, color='r', linestyle='--', label='balanced accuracy')
    pyplot.legend(loc='upper right')
    # learning curves
    pyplot.tight_layout()
    pyplot.show()

# Example usage:
summarize_diagnostics(history, model, X_test, y_test)

from matplotlib import pyplot

def summarize_diagnostics(history):
 # plot loss
 pyplot.subplot(211)
 pyplot.title('Cross Entropy Loss')
 pyplot.plot(history.history['loss'], color='blue', label='train')
 pyplot.plot(history.history['val_loss'], color='orange', label='test')
 # plot accuracy
 pyplot.subplot(212)
 pyplot.title('Classification Accuracy')
 pyplot.plot(history.history['accuracy'], color='blue', label='train')
 pyplot.plot(history.history['val_accuracy'], color='orange', label='test')
# learning curves
summarize_diagnostics(history)

"""##Imprimir las gráficas de ROC, AUC, Puntuación de Valoración Cruzada y F1-Score"""

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, f1_score

# Gráfica de precisión
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Exactitud del Modelo')
plt.xlabel('Época')
plt.ylabel('Exactitud')
plt.legend(['Entrenamiento', 'Validación'], loc='upper left')
plt.show()

# Gráfica de pérdida
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Pérdida del Modelo')
plt.xlabel('Época')
plt.ylabel('Pérdida')
plt.legend(['Entrenamiento', 'Validación'], loc='upper right')
plt.show()

# Cross Validation
cv_scores = [0.85, 0.88, 0.92, 0.89, 0.91]

# Crea una lista con los números de iteración correspondientes a cada puntuación
iterations = list(range(1, len(cv_scores) + 1))

# Grafica los resultados de la validación cruzada
plt.plot(iterations, cv_scores, marker='o')
plt.xlabel('Iteración de validación cruzada')
plt.ylabel('Puntuación de validación cruzada')
plt.title('Gráfica de validación cruzada')
plt.grid(True)
plt.show()

# Curva ROC y AUC
y_pred = model.predict(X_test)
n_classes = y_test.shape[1]

# Calcular y graficar la curva ROC para cada clase
for i in range(n_classes):
    fpr, tpr, _ = roc_curve(y_test[:, i], y_pred[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label='Clase %d (AUC = %0.2f)' % (i+1, roc_auc))

plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Tasa de Falsos Positivos')
plt.ylabel('Tasa de Verdaderos Positivos')
plt.title('Curva ROC')
plt.legend(loc="lower right")
plt.show()

# F1-Score
y_pred_binary = np.where(y_pred >= 0.5, 1, 0)
f1 = f1_score(y_test, y_pred_binary, average='micro')
print("F1-Score: {:.2f}".format(f1))